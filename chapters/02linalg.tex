\section{Linear Algebra Basics}
\subsection{Scalars, Vectors, Matrices, and Tensors}
\begin{itemize}
    \item \textbf{Scalar}: single number, specified by type $\mathbb{R}$, $\mathbb{N}$, $\mathbb{Z}$
    \item \textbf{Vector}: an array of numbers arranged in a single row or column 
    \begin{itemize}
        \item First element of $\mathbf{x}$ is $x_1$, second is $x_2$, and so on:
        $$\begin{bmatrix}
            x_1 \\
            x_2 \\
            \vdots \\
            x_n \\
        \end{bmatrix}$$
        \item must specify the type of numbers stored, i.e., $\mathbf{x} \in \mathbb{R}^n$, where $n$ is the number of elements/dimensionality
        \item can think of a vector as identifying a point in space; each element gives a coordinate along a different axis
        \item can index vectors with a set
        \begin{itemize}
            \item indices $1, 3, 6$ $\rightarrow$ $S = \{1, 3, 6\}$ $\rightarrow$ $x_S = \{x_1, x_3, x_6\}$
        \end{itemize}
        \item "$-$" indicates the complement of a set; $x_{-1} \rightarrow$ all elements except $-1$
    \end{itemize}
    \item \textbf{Matrix}: 2-d array of numbers
    \begin{itemize}
        \item each element is specified by two indices (row, col) instead of one
        \item $A_{m,n}$: entry at row $m$, col $n$
        \item $A_{i, :}$: all entries in the $i_{th}$ row of $A$
        \item $A_{:, j}$: all entries in the $j_{th}$ column of $A$
    \end{itemize}
    \item \textbf{Tensor}: Array with more than two axes
    \begin{itemize}
        \item $A_{i, j, k}$
    \end{itemize}
    \item \textbf{Transpose}: mirror image of a matrix across its main diagonal
    \begin{itemize}
        \item $(A^T)_{i,j} = A_{j, i}$
        \item row-column swap
    \end{itemize}
    \item \textbf{Matrix Addition}: element-wise addition of two matrices of the same size
    \item Scalar times matrix: $D = a \cdot B + c$ $\rightarrow$ $D_{i,j} = a \cdot B_{i,j} + c$ 
    \item Matrix-Vector Addition: $C = A + \mathbf{b}$ $\rightarrow$  $C_{i, j} = A_{i, j} + b_j$
    \begin{itemize}
        \item vector b is added to each row of matrix A
        \item \textbf{Broadcasting}: the copying of a vector to match the dimensions of a matrix
    \end{itemize}
\end{itemize}

\subsection{Matrix and Vector Multiplication}
\begin{itemize}
    \item \textbf{matrix product}: $C = AB$
    \begin{itemize}
        \item to be defined, $A$, must have the same number of columns as $B$ has rows.
        \item if $A$ is $m \times n$ and $B$ is $n \times p$, then $C$ is shape $m \times p$
        \item $C_{i,j} = \sum_k A_{i,k} B_{k,j}$
    \end{itemize}
    \item \textbf{Hadamard product}: element-wise multiplication of a matrix, denoted $A \circ B$
    \item \textbf{Dot Product}: $x \cdot y$ is the same dimensionality as the matrix product $x^T y$
    \begin{itemize}
        \item $C = AB$ $\rightarrow$ $C_{i,j}$ is the dot product of row $i$ of $A$ and column $j$ of $B$ 
    \end{itemize}
    \item \textbf{Matrix Operation Properties}:
    \begin{itemize}
        \item distributive: $A(B + C) = AB + AC$
        \item associative: $(AB)C = A(BC)$
        \item \textbf{not} commutative: $AB \neq BA$ in general; however vector dot product is commutative: $x^T y = y^T x$
        \item transpose of a matrix product: $(AB)^T = B^T A^T$
    \end{itemize}
    \item \textbf{System of Linear Equations}
    \begin{itemize}
        \item $Ax=b$, $A \in \mathbb{R}^{m \times n}$ is a known matrix, $b \in \mathbb{R}^m$ is a known vector, and $x \in \mathbb{R}^n$ is unkown, and we would like to solve for it.
        \item can rewrite as: $$A_{1,1}x_1 + A_{1,2}x_2 + \dots + A_{1, n}x_n = b_1$$
        $$A_{2,1}x_1 + A_{2,2}x_2 + \dots + A_{2, n}x_n = b_2$$ 
        $$\vdots$$
        $$A_{m,1}x_1 + A_{m,2}x_2 + \dots + A_{m, n}x_n = b_m$$
    \end{itemize}
\end{itemize}

\subsection{Identity and Inverse Matrices}
\begin{itemize}
    \item \textbf{Matrix Inversion}: analytic solution to the system of linear equations
    \item \textbf{Identity Matrix}: does not change a vector when multiplied by it; denoted $I_n$, formally $I_n \in \mathbb{R^{n \times n}}$ and $\forall x \in \mathbb{R}^n, I_n x = x$; takes form of 1s along the diagonal
    \item \textbf{Matrix Inverse}: $A^{-1}$ is the inverse of $A$, defined as the matrix such that $A^{-1}A = I_n$
    \item Solving a system of linear equations:
    \begin{align}
        Ax &= b \\
        A^{-1}Ax &= A^{-1}b \\
        I_n x &= A^{-1}b \\
        x &= A^{-1}b 
    \end{align}
    \item inverse matrix is primarily used as a theoretical tool - it's hard to represent at high precision on a digital computer
\end{itemize}

\subsection{Linear Dependence and Span}
\begin{itemize}
    \item for $A^{-1}$ to exist, the system of lin. eqns. must have one solution for every value of $b$
    \item it is also possible to have a system with no solutions or infinitely many solutions
    \item it is not possible to have a system with more than one but less than infinite solutions;  if $x$ and $y$ are solutions, then 
    $$z = \alpha x + (1 - \alpha) y$$
    is also a solution $\forall \alpha \in \mathbb{R}$
    \item \textbf{Finding Solutions to a System of Linear Equations}:
    \begin{itemize}
        \item columns of $A$: different directions that we can travel from the origin
        \item how many ways are there to reach $b$?
        \item $x$ how far we travel in each direction:
        $$ Ax = \sum_i x_i A_{:, i}$$
        \item textbf{Linear Combination}: multiply a set of vectors by scalars and add the results:
        $$ \sum_i c_i v^{i} $$
        \item \textbf{Span}: all points obtainable by linear combination of the original vectors
        \item to find if the system has a solution, we must check if $b$ is in the span of the columns of $A$, called the \textbf{column space/range} of $A$.
        \item $A$ must have at least $m$ columns to span $\mathbb{R}^m$; if $A$ has fewer than $m$ columns, then it cannot span $\mathbb{R}^m$ and the system has no solutions
        \item columns can also be redundant; so this is not a sufficient condition for a solution
        \item \textbf{Linear Dependence}: if one column can be expressed as a linear combination of other columns 
        \item \textbf{Linear Independence}: no vector in the set is a linear combination of the other vectors
        \item for a column space to span $\mathbb{R}^m$, it must have at least $m$ linearly independent columns
    \end{itemize}
    \item for a matrix to have an inverse, we need to ensure that our system has $at most$ one solution for each value of $b$; the matrix then must have at most $m$ columns, otherwise one of the columns must be linearly dependent
    \item therefore, our matrix must be square ($m = n$), and all columns must be linearly independent; this is a \textbf{singular} matrix
\end{itemize}

\subsection{Norms}
\begin{itemize}
    \item \textbf{Norm}: a way to measure the 'size' of a vector. Functions mapping vectors to non-negative values. The $L^p$ norm is given by: 
    $$ ||x||_p = \Big( \sum_i |x_i|^p \Big)^{\frac{1}{p}}, p \in \mathbb{R}, p \geq 1 $$
    \item intuitively, norms measure the distance of a vector from the origin
    \item rigorously, norms must satisfy the following properties:
    \begin{itemize}
        \item $f(x) = 0 \Rightarrow x = 0$ (only the zero vector has a norm of zero)
        \item $f(x + y) \leq f(x) + f(y)$ (triangle inequality) (the norm of the sum is less than or equal to the sum of the norms)
        \item $\forall \alpha \in \mathbb{R}, f(\alpha x) = |\alpha| f(x)$ (scaling property) (the norm of a scaled vector is the absolute value of the scaling factor times the norm of the vector)
    \end{itemize}
    \item \textbf{Euclidean Norm}: $L^2$ norm 
    \begin{itemize}
        \item the euclidean distance from the origin to a point. 
        \item Often denoted as $||x||_2$ or even $||x||$.
        \item Additionally, can measure the vector size with the squared $L^2$ norm, $x^T x$.
        \item derivative of squared $L^2$ wrt $x_n$ only depends on the $n$-th component of $x$.
        \item increases slowly near the origin, and quickly as we move away from it.
    \end{itemize}
    \item \textbf{Manhattan Distance}: $L^1$ Norm
    \begin{itemize}
        \item the sum of absolute values of a vector.
        \item grows at the same rate in all locations (no exponent)
        \item used in ML when the difference between zero and nonzero values are important
    \end{itemize}
    \item \textbf{Max Norm}: $L^\infty$ Norm
    \begin{itemize}
        \item the absolute value of the largest component of a vector
        \item $||x||_\infty = \max_i |x_i|$
    \end{itemize}
    \item \textbf{Frobenius Norm}: $L^2$ norm of a matrix
    \begin{itemize}
        \item measures the size of a matrix
        $$ ||A||_F = \sqrt{\sum_{i,j} A_{i,j}^2} $$
    \end{itemize}
    \item can rewrite the dot produce in terms of norms:
    $$ x^T y = ||x||_2 ||y||_2 \cos(\theta) $$
    where $\theta$ is the angle between the two vectors.
\end{itemize}

\subsection{Special Kinds of Matrices and Vectors}
\begin{itemize}
    \item \textbf{Diagonal Matrices}: only have nonzero entries along the main diagonal
    \begin{itemize}
        \item denoted as $diag(v)$, where $v$ is the vector of diagonal entries
        \item to compute $diag(v)x$, we multiply each element $x_i$ by $v_i$; $diag(v)x = v \circ x$
        \item inverse exists if all diagonal entries are nonzero, and if so,
        $$ diag(v)^{-1} = diag([1/v_1, ..., 1/v_n]^T) $$
        \item diagonal matrices can be non-square; don't have inverses, but can still multiply cheaply
    \end{itemize}
    \item \textbf{Symmetric Matrices}: any matrix that is equal to its own transpose: $A^T = A$
    \begin{itemize}
        \item often occur when entreis are generate by a function with two arguments that doesn't depend on argument order
        \item ex: $A$ is a distance matrix, where $A_{i,j}$ is the distance between $i$ and $j$, so $A_{i,j} = A_{j,i}$, because distance functions are symmetric.
    \end{itemize}
    \item \textbf{Unit Vector}: a vector with a unit norm, i.e. $||x||_2 = 1$.
    \item vectors $x$ and $y$ are \textbf{orthogonal} if $x^T y = 0$; if both vectors have a nonzero norm, this means that they are at a 90 degree angle from each other 
    \item \textbf{Orthonormal Vectors}: orthogonal vectors with a unit norm
    \item \textbf{Orthogonal Matrix}: a square matrix with rows that mutually orthonormal and columns that are mutually orthonormal:
    $$ A^T A = AA^T = I; A^{-1} = A^T $$
    \item orthogonal matrices are of interest because their inverse is easy to compute
\end{itemize}

\subsection{Eigendecomposition}
\begin{itemize}
    \item
\end{itemize}

\subsection{Singular Value Decomposition}
\begin{itemize}
    \item
\end{itemize}

\subsection{Moore-Penrose Pseudoinverse}
\begin{itemize}
    \item
\end{itemize}

\subsection{Trace Operator}
\begin{itemize}
    \item
\end{itemize}

\subsection{Determinant}
\begin{itemize}
    \item
\end{itemize}

\subsection{Example: PCA}

