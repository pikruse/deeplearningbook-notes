\section{Linear Algebra Basics}
\subsection{Scalars, Vectors, Matrices, and Tensors}
\begin{itemize}
    \item \textbf{Scalar}: single number, specified by type $\mathbb{R}$, $\mathbb{N}$, $\mathbb{Z}$
    \item \textbf{Vector}: an array of numbers arranged in a single row or column 
    \begin{itemize}
        \item First element of $\mathbf{x}$ is $x_1$, second is $x_2$, and so on:
        $\begin{pmatrix}
            x_1 \\
            x_2 \\
            \vdots \\
            x_n \\
        \end{pmatrix}$
        \item must specify the type of numbers stored, i.e., $\mathbf{x} \in \mathbb{R}^n$, where $n$ is the number of elements/dimensionality
        \item can think of a vector as identifying a point in space; each element gives a coordinate along a different axis
        \item can index vectors with a set
        \begin{itemize}
            \item indices $1, 3, 6$ $\rightarrow$ $S = \{1, 3, 6\}$ $\rightarrow$ $x_S = \{x_1, x_3, x_6\}$
        \end{itemize}
        \item "$-$" indicates the complement of a set; $x_{-1} \rightarrow$ all elements except $-1$
    \end{itemize}
    \item \textbf{Matrix}: 2-d array of numbers
    \begin{itemize}
        \item each element is specified by two indices (row, col) instead of one
        \item $A_{m,n}$: entry at row $m$, col $n$
        \item $A_{i, :}$: all entries in the $i_{th}$ row of $A$
        \item $A_{:, j}$: all entries in the $j_{th}$ column of $A$
    \end{itemize}
    \item \textbf{Tensor}: Array with more than two axes
    \begin{itemize}
        \item $A_{i, j, k}$
    \end{itemize}
    \item \textbf{Transpose}: mirror image of a matrix across its main diagonal
    \begin{itemize}
        \item $(A^T)_{i,j} = A_{j, i}$
        \item row-column swap
    \end{itemize}
    \item \textbf{Matrix Addition}: element-wise addition of two matrices of the same size
    \item Scalar times matrix: $D = a \cdot B + c$ $\rightarrow$ $D_{i,j} = a \cdot B_{i,j} + c$ 
    \item Matrix-Vector Addition: $C = A + \mathbf{b}$ $\rightarrow$  $C_{i, j} = A_{i, j} + b_j$
    \begin{itemize}
        \item vector b is added to each row of matrix A
        \item \textbf{Broadcasting}: the copying of a vector to match the dimensions of a matrix
    \end{itemize}
\end{itemize}

\subsection{Matrix and Vector Multiplication}
\begin{itemize}
    \item \textbf{matrix product}: $C = AB$
    \begin{itemize}
        \item to be defined, $A$, must have the same number of columns as $B$ has rows.
        \item if $A$ is $m \times n$ and $B$ is $n \times p$, then $C$ is shape $m \times p$
        \item $C_{i,j} = \sum_k A_{i,k} B_{k,j}$
    \end{itemize}
    \item \textbf{Hadamard product}: element-wise multiplication of a matrix, denoted $A \circ B$
    \item \textbf{Dot Product}: $x \cdot y$ is the same dimensionality as the matrix product $x^T y$
    \begin{itemize}
        \item $C = AB$ $\rightarrow$ $C_{i,j}$ is the dot product of row $i$ of $A$ and column $j$ of $B$ 
    \end{itemize}
    \item \textbf{Matrix Operation Properties}:
    \begin{itemize}
        \item distributive: $A(B + C) = AB + AC$
        \item associative: $(AB)C = A(BC)$
        \item \textbf{not} commutative: $AB \neq BA$ in general; however vector dot product is commutative: $x^T y = y^T x$
        \item transpose of a matrix product: $(AB)^T = B^T A^T$
    \end{itemize}
    \item \textbf{System of Linear Equations}
    \begin{itemize}
        \item $Ax=b$, $A \in \mathbb{R}^{m \times n}$ is a known matrix, $b \in \mathbb{R}^m$ is a known vector, and $x \in \mathbb{R}^n$ is unkown, and we would like to solve for it.
        \item can rewrite as: $$A_{1,1}x_1 + A_{1,2}x_2 + \dots + A_{1, n}x_n = b_1$$
        $$A_{2,1}x_1 + A_{2,2}x_2 + \dots + A_{2, n}x_n = b_2$$ 
        $$\vdots$$
        $$A_{m,1}x_1 + A_{m,2}x_2 + \dots + A_{m, n}x_n = b_m$$
    \end{itemize}
\end{itemize}

\subsection{Identity and Inverse Matrices}
\begin{itemize}
    \item \textbf{Matrix Inversion}: analytic solution to the system of linear equations
    \item \textbf{Identity Matrix}: does not change a vector when multiplied by it; denoted $I_n$, formally $I_n \in \mathbb{R^{n \times n}}$ and $\forall x \in \mathbb{R}^n, I_n x = x$; takes form of 1s along the diagonal
    \item \textbf{Matrix Inverse}: $A^{-1}$ is the inverse of $A$, defined as the matrix such that $A^{-1}A = I_n$
    \item Solving a system of linear equations:
    \begin{align}
        Ax &= b \\
        A^{-1}Ax &= A^{-1}b \\
        I_n x &= A^{-1}b \\
        x &= A^{-1}b 
    \end{align}
    \item inverse matrix is primarily used as a theoretical tool - it's hard to represent at high precision on a digital computer
\end{itemize}

\subsection{Linear Dependence and Span}
\begin{itemize}
    \item for $A^{-1}$ to exist, the system of lin. eqns. must have one solution for every value of $b$
    \item it is also possible to have a system with no solutions or infinitely many solutions
    \item it is not possible to have a system with more than one but less than infinite solutions;  if $x$ and $y$ are solutions, then 
    $$z = \alpha x + (1 - \alpha) y$$
    is also a solution $\forall \alpha \in \mathbb{R}$
    \item \textbf{Finding Solutions to a System of Linear Equations}:
    \begin{itemize}
        \item columns of $A$: different directions that we can travel from the origin
        \item how many ways are there to reach $b$?
        \item $x$ how far we travel in each direction:
        $$ Ax = \sum_i x_i A_{:, i}$$
        \item textbf{Linear Combination}: multiply a set of vectors by scalars and add the results:
        $$ \sum_i c_i v^{i} $$
        \item \textbf{Span}: all points obtainable by linear combination of the original vectors
        \item to find if the system has a solution, we must check if $b$ is in the span of the columns of $A$, called the \textbf{column space/range} of $A$.
        \item $A$ must have at least $m$ columns to span $\mathbb{R}^m$; if $A$ has fewer than $m$ columns, then it cannot span $\mathbb{R}^m$ and the system has no solutions
        \item columns can also be redundant; so this is not a sufficient condition for a solution
        \item \textbf{Linear Dependence}: if one column can be expressed as a linear combination of other columns 
        \item \textbf{Linear Independence}: no vector in the set is a linear combination of the other vectors
        \item for a column space to span $\mathbb{R}^m$, it must have at least $m$ linearly independent columns
    \end{itemize}
    \item for a matrix to have an inverse, we need to ensure that our system has $at most$ one solution for each value of $b$; the matrix then must have at most $m$ columns, otherwise one of the columns must be linearly dependent
    \item therefore, our matrix must be square ($m = n$), and all columns must be linearly independent; this is a \textbf{singular} matrix
\end{itemize}

\subsection{Norms}
\begin{itemize}
    \item 
\end{itemize}

\subsection{Special Kinds of Matrices and Vectors}
\begin{itemize}
    \item 
\end{itemize}

\subsection{Eigendecomposition}
\begin{itemize}
    \item
\end{itemize}

\subsection{Singular Value Decomposition}
\begin{itemize}
    \item
\end{itemize}

\subsection{Moore-Penrose Pseudoinverse}
\begin{itemize}
    \item
\end{itemize}

\subsection{Trace Operator}
\begin{itemize}
    \item
\end{itemize}

\subsection{Determinant}
\begin{itemize}
    \item
\end{itemize}

\subsection{Example: PCA}

