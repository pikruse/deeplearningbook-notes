\section{Linear Algebra Basics}
\subsection{Scalars, Vectors, Matrices, and Tensors}
\begin{itemize}
    \item \textbf{Scalar}: single number, specified by type $\mathbb{R}$, $\mathbb{N}$, $\mathbb{Z}$.
    \item \textbf{Vector}: an array of numbers arranged in a single row or column 
    \begin{itemize}
        \item First element of $\mathbf{x}$ is $x_1$, second is $x_2$, and so on:
        $$\begin{bmatrix}
            x_1 \\
            x_2 \\
            \vdots \\
            x_n \\
        \end{bmatrix}$$
        \item must specify the type of numbers stored, i.e., $\mathbf{x} \in \mathbb{R}^n$, where $n$ is the number of elements/dimensionality
        \item can think of a vector as identifying a point in space; each element gives a coordinate along a different axis
        \item can index vectors with a set
        \begin{itemize}
            \item indices $1, 3, 6$ $\rightarrow$ $S = \{1, 3, 6\}$ $\rightarrow$ $x_S = \{x_1, x_3, x_6\}$
        \end{itemize}
        \item "$-$" indicates the complement of a set; $x_{-1} \rightarrow$ all elements except $-1$
    \end{itemize}
    \item \textbf{Matrix}: 2-d array of numbers
    \begin{itemize}
        \item each element is specified by two indices (row, col) instead of one
        \item $A_{m,n}$: entry at row $m$, col $n$
        \item $A_{i, :}$: all entries in the $i_{th}$ row of $A$
        \item $A_{:, j}$: all entries in the $j_{th}$ column of $A$
    \end{itemize}
    \item \textbf{Tensor}: Array with more than two axes
    \begin{itemize}
        \item $A_{i, j, k}$
    \end{itemize}
    \item \textbf{Transpose}: mirror image of a matrix across its main diagonal
    \begin{itemize}
        \item $(A^T)_{i,j} = A_{j, i}$
        \item row-column swap
    \end{itemize}
    \item \textbf{Matrix Addition}: element-wise addition of two matrices of the same size
    \item Scalar times matrix: $D = a \cdot B + c$ $\rightarrow$ $D_{i,j} = a \cdot B_{i,j} + c$ 
    \item Matrix-Vector Addition: $C = A + \mathbf{b}$ $\rightarrow$  $C_{i, j} = A_{i, j} + b_j$
    \begin{itemize}
        \item vector b is added to each row of matrix A
        \item \textbf{Broadcasting}: the copying of a vector to match the dimensions of a matrix
    \end{itemize}
\end{itemize}

\subsection{Matrix and Vector Multiplication}
\begin{itemize}
    \item \textbf{matrix product}: $C = AB$
    \begin{itemize}
        \item to be defined, $A$, must have the same number of columns as $B$ has rows.
        \item if $A$ is $m \times n$ and $B$ is $n \times p$, then $C$ is shape $m \times p$
        \item $C_{i,j} = \sum_k A_{i,k} B_{k,j}$
    \end{itemize}
    \item \textbf{Hadamard product}: element-wise multiplication of a matrix, denoted $A \circ B$
    \item \textbf{Dot Product}: $x \cdot y$ is the same dimensionality as the matrix product $x^T y$
    \begin{itemize}
        \item $C = AB$ $\rightarrow$ $C_{i,j}$ is the dot product of row $i$ of $A$ and column $j$ of $B$ 
    \end{itemize}
    \item \textbf{Matrix Operation Properties}:
    \begin{itemize}
        \item distributive: $A(B + C) = AB + AC$
        \item associative: $(AB)C = A(BC)$
        \item \textbf{not} commutative: $AB \neq BA$ in general; however vector dot product is commutative: $x^T y = y^T x$
        \item transpose of a matrix product: $(AB)^T = B^T A^T$
    \end{itemize}
    \item \textbf{System of Linear Equations}
    \begin{itemize}
        \item $Ax=b$, $A \in \mathbb{R}^{m \times n}$ is a known matrix, $b \in \mathbb{R}^m$ is a known vector, and $x \in \mathbb{R}^n$ is unkown, and we would like to solve for it.
        \item can rewrite as: $$A_{1,1}x_1 + A_{1,2}x_2 + \dots + A_{1, n}x_n = b_1$$
        $$A_{2,1}x_1 + A_{2,2}x_2 + \dots + A_{2, n}x_n = b_2$$ 
        $$\vdots$$
        $$A_{m,1}x_1 + A_{m,2}x_2 + \dots + A_{m, n}x_n = b_m$$
    \end{itemize}
\end{itemize}

\subsection{Identity and Inverse Matrices}
\begin{itemize}
    \item \textbf{Matrix Inversion}: analytic solution to the system of linear equations
    \item \textbf{Identity Matrix}: does not change a vector when multiplied by it; denoted $I_n$, formally $I_n \in \mathbb{R^{n \times n}}$ and $\forall x \in \mathbb{R}^n, I_n x = x$; takes form of 1s along the diagonal
    \item \textbf{Matrix Inverse}: $A^{-1}$ is the inverse of $A$, defined as the matrix such that $A^{-1}A = I_n$
    \item Solving a system of linear equations:
    \begin{align}
        Ax &= b \\
        A^{-1}Ax &= A^{-1}b \\
        I_n x &= A^{-1}b \\
        x &= A^{-1}b 
    \end{align}
    \item inverse matrix is primarily used as a theoretical tool - it's hard to represent at high precision on a digital computer
\end{itemize}

\subsection{Linear Dependence and Span}
\begin{itemize}
    \item for $A^{-1}$ to exist, the system of lin. eqns. must have one solution for every value of $b$
    \item it is also possible to have a system with no solutions or infinitely many solutions
    \item it is not possible to have a system with more than one but less than infinite solutions;  if $x$ and $y$ are solutions, then 
    $$z = \alpha x + (1 - \alpha) y$$
    is also a solution $\forall \alpha \in \mathbb{R}$
    \item \textbf{Finding Solutions to a System of Linear Equations}:
    \begin{itemize}
        \item columns of $A$: different directions that we can travel from the origin
        \item how many ways are there to reach $b$?
        \item $x$ how far we travel in each direction:
        $$ Ax = \sum_i x_i A_{:, i}$$
        \item textbf{Linear Combination}: multiply a set of vectors by scalars and add the results:
        $$ \sum_i c_i v^{i} $$
        \item \textbf{Span}: all points obtainable by linear combination of the original vectors
        \item to find if the system has a solution, we must check if $b$ is in the span of the columns of $A$, called the \textbf{column space/range} of $A$.
        \item $A$ must have at least $m$ columns to span $\mathbb{R}^m$; if $A$ has fewer than $m$ columns, then it cannot span $\mathbb{R}^m$ and the system has no solutions
        \item columns can also be redundant; so this is not a sufficient condition for a solution
        \item \textbf{Linear Dependence}: if one column can be expressed as a linear combination of other columns 
        \item \textbf{Linear Independence}: no vector in the set is a linear combination of the other vectors
        \item for a column space to span $\mathbb{R}^m$, it must have at least $m$ linearly independent columns
    \end{itemize}
    \item for a matrix to have an inverse, we need to ensure that our system has $at most$ one solution for each value of $b$; the matrix then must have at most $m$ columns, otherwise one of the columns must be linearly dependent
    \item therefore, our matrix must be square ($m = n$), and all columns must be linearly independent; this is a \textbf{singular} matrix
\end{itemize}

\subsection{Norms}
\begin{itemize}
    \item \textbf{Norm}: a way to measure the 'size' of a vector. Functions mapping vectors to non-negative values. The $L^p$ norm is given by: 
    $$ ||x||_p = \Big( \sum_i |x_i|^p \Big)^{\frac{1}{p}}, p \in \mathbb{R}, p \geq 1 $$
    \item intuitively, norms measure the distance of a vector from the origin
    \item rigorously, norms must satisfy the following properties:
    \begin{itemize}
        \item $f(x) = 0 \Rightarrow x = 0$ (only the zero vector has a norm of zero)
        \item $f(x + y) \leq f(x) + f(y)$ (triangle inequality) (the norm of the sum is less than or equal to the sum of the norms)
        \item $\forall \alpha \in \mathbb{R}, f(\alpha x) = |\alpha| f(x)$ (scaling property) (the norm of a scaled vector is the absolute value of the scaling factor times the norm of the vector)
    \end{itemize}
    \item \textbf{Euclidean Norm}: $L^2$ norm 
    \begin{itemize}
        \item the euclidean distance from the origin to a point. 
        \item Often denoted as $||x||_2$ or even $||x||$.
        \item Additionally, can measure the vector size with the squared $L^2$ norm, $x^T x$.
        \item derivative of squared $L^2$ wrt $x_n$ only depends on the $n$-th component of $x$.
        \item increases slowly near the origin, and quickly as we move away from it.
    \end{itemize}
    \item \textbf{Manhattan Distance}: $L^1$ Norm
    \begin{itemize}
        \item the sum of absolute values of a vector.
        \item grows at the same rate in all locations (no exponent)
        \item used in ML when the difference between zero and nonzero values are important
    \end{itemize}
    \item \textbf{Max Norm}: $L^\infty$ Norm
    \begin{itemize}
        \item the absolute value of the largest component of a vector
        \item $||x||_\infty = \max_i |x_i|$
    \end{itemize}
    \item \textbf{Frobenius Norm}: $L^2$ norm of a matrix
    \begin{itemize}
        \item measures the size of a matrix
        $$ ||A||_F = \sqrt{\sum_{i,j} A_{i,j}^2} $$
    \end{itemize}
    \item can rewrite the dot produce in terms of norms:
    $$ x^T y = ||x||_2 ||y||_2 \cos(\theta) $$
    where $\theta$ is the angle between the two vectors.
\end{itemize}

\subsection{Special Kinds of Matrices and Vectors}
\begin{itemize}
    \item \textbf{Diagonal Matrices}: only have nonzero entries along the main diagonal
    \begin{itemize}
        \item denoted as $diag(v)$, where $v$ is the vector of diagonal entries
        \item to compute $diag(v)x$, we multiply each element $x_i$ by $v_i$; $diag(v)x = v \circ x$
        \item inverse exists if all diagonal entries are nonzero, and if so,
        $$ diag(v)^{-1} = diag([1/v_1, ..., 1/v_n]^T) $$
        \item diagonal matrices can be non-square; don't have inverses, but can still multiply cheaply
    \end{itemize}
    \item \textbf{Symmetric Matrices}: any matrix that is equal to its own transpose: $A^T = A$
    \begin{itemize}
        \item often occur when entreis are generate by a function with two arguments that doesn't depend on argument order
        \item ex: $A$ is a distance matrix, where $A_{i,j}$ is the distance between $i$ and $j$, so $A_{i,j} = A_{j,i}$, because distance functions are symmetric.
    \end{itemize}
    \item \textbf{Unit Vector}: a vector with a unit norm, i.e. $||x||_2 = 1$.
    \item vectors $x$ and $y$ are \textbf{orthogonal} if $x^T y = 0$; if both vectors have a nonzero norm, this means that they are at a 90 degree angle from each other 
    \item \textbf{Orthonormal Vectors}: orthogonal vectors with a unit norm
    \item \textbf{Orthogonal Matrix}: a square matrix with rows that mutually orthonormal and columns that are mutually orthonormal:
    $$ A^T A = AA^T = I; A^{-1} = A^T $$
    \item orthogonal matrices are of interest because their inverse is easy to compute
\end{itemize}

\subsection{Eigendecomposition}
\textbf{Intution:} you can think of a matrix as a function that transforms space (or individual vectors). However, some special vectors are invariant to this transformation. If a vector only gets scaled by a matrix, and not rotated, then it's an eigenvector. This scaling factor is an eigenvalue. The "unchanged" directions in the matrix tranformation are a fundamental property of that matrix.
\begin{itemize}
    \item eigendecomposition is a way to decompose matrices to find information about their functional properties
    \item eigenvalues represent the direction that space is being transformed; eigenvalues represent the magnitude
    \item \textbf{Eigendecomposition}: decompose a matrix into eigenvectors and eigenvalues
    \item \textbf{Eigenvector}: of $A$; a nonzero vector $v$ such that multipcation by $A$ alters only the scale of $v$:
    $$ A v = \lambda v$$
    \item The scalar $\lambda$ is the \textbf{eignvalue}
    \begin{itemize}
        \item if $v$ is an eigenvector of $A$, then any scaled vector $sv$ is also one
        \item $sv$ has the same eigenvalue as $v$
        \item therefore, we usually only look at unit eigenvectors
    \end{itemize}
    \item a group of linearly independent eigenvectors ${v^1, \dots, v^n}$ with eigenvalues ${\lambda^1, \dots, \lambda^n}$ can be written as a matrix $V$:
    $$ V = [v^1, \dots, v^n]$$
    \item we can make a vector of eigenvalues as well:
    $$ \lambda = \begin{bmatrix}
        \lambda^1 \\
        \vdots \\
        \lambda^n 
    \end{bmatrix}$$
    \item the \textbf{eigendecomposition} of $A$ is:
    $$ A = V \text{diag}(\lambda)V^{-1}$$
    \item \textit{constructing} matrices with specfic eigenvalues/vectors allows us to stretch space in desired directions
    \item \textbf{Decomposing} matrices into eigenvectors/values helps us analyze their properties
    \item Every real, symmetric matrix can be decomposed into an expression of real-valued eigenvectors and eigenvalues:
    $$ A = Q \Lambda Q^T$$,
    where $Q$ is an orthogonal matrix of eigenvectors of $A$, and $\Lambda$ is a diagonal matrix.
    \item Eigendecomposition of a real symmetric matrix can also be used to optimize quadratic expressions of the form $f(x) = x^T Ax $ subject to $||x||_2 = 1$.
    \item \textbf{Positive Definite:} a matrix whose eigenvalues are all positive
    \item \textbf{Positive Semidefinite:} a matrix whose eigenvalues are all non-negative (positive or zero). Guarantee that $x^T Ax \geq 0 \forall x$
    \item \textbf{Negative Semidefinite:} a matrix whose eigenvalues are negative or zero-valued
\end{itemize}

\subsection{Singular Value Decomposition}
\begin{itemize}
    \item \textbf{Singular Value Decompostion} factors a matrix into \textbf{singular vectors} and \textbf{singular values}
    \item more generally applicable than eigendecomposition; can be applied to \textit{any real matrix}
    \item Eigendecomposition:
    $$ A = V diag(\lambda)V^{-1}$$
    \item SVD:
    $$ A = UDV^T$$
    \item if $A$ is a $m \times n$ matrix, then $U$ is a $m \times m$ matrix, $D$ is an $m \times n$ matrix, and $V$ is an $n \times n$ matrix
    \item SVD Matrix shapes:
    \begin{itemize}
        \item $U$ and $V$ are orthogonal matrices
        \item $D$ is a diagonal matrix
    \end{itemize}
    \item elements along the diagonal of $D$ are the \textbf{singular values} of $A$
    \item columns of $U$ are the \textbf{left singular vectors}, columns of $V$ are the \textbf{right singular vectors}
    \item Relationship to eigendecomposition:
    \begin{itemize}
        \item left singular vectors are the eigenvectors of $AA^T$
        \item right singular vectors are the eigenvectors of $A^TA$
        \item non-zero singular values are the square roots of the eigenvalues of $A^TA$ or $AA^T$
    \end{itemize}
    \item \textit{can utilize SVD to partially generalize matrix inversion to non-square matrices}
\end{itemize}

\subsection{Moore-Penrose Pseudoinverse}
\begin{itemize}
    \item matrix inversion is not defined for matrices that are not square
    \item \textbf{Moore-Penrose Pseudoinverse}:
    $$ A^+ = \lim_{a \rightarrow 0}(A^TA + \alpha I)^{-1}A^T$$
    \item practical formula for computing MPP:
    $$ A^+ = V D^+ U^T$$
    where $U$, $D$, and $V$ are the SVD of $A$, and $D^+$ is the pseudoinverse of $D$, obtained by taking the reciprocal of its nonzero elements and tranposing the resulting matrix
    \item if $A$ has more columns than rows, it is possible to find a solution, or many
    \item if $A$ has more rows than columns, it is possible for there to be no solution, but we can find the $x$ as close as possible to $y$ in terms of Euclidean norm
\end{itemize}

\subsection{Trace Operator}
\begin{itemize}
    \item The \textbf{Trace} gives the sum of all diagonal entries of a matrix:
    $$ Tr(A) = \sum_i A_{i,i}$$
    \item allows us to write some operations without summation notation, which allows us to manipulate the expression with identities 
    \item \textbf{Trace Properties}:
    \begin{itemize}
        \item $Tr(A) = Tr(A^T)$: trace is invariant to transposition
        \item $Tr(ABC) = Tr(CAB) = Tr(BCA)$: trace is invariant to moving last matrix factor to the first position
        \item $a = Tr(a)$: trace of a scalar is the scalar itself
    \end{itemize}
\end{itemize}

\subsection{Determinant}
\begin{itemize}
    \item \textbf{determinant}: function mapping matrices to real scalars; equal to the product of all eigenvalues of the matrix
    \item can be thought of as a measure of how much matrix multiplication expands or contracts space
    \item $det(A) = 0$, then space is contracted completely along at least one dimension
    \item $det(A) = 1$, transformation preserves "volume" (dimensionality) of the space
\end{itemize}

\subsection{Example: PCA}
\begin{itemize}
    \item begin with $m$ points ${x^1, \dots, x^m} \in \mathbb{R}^n$.
    \item goal: store the points in a way that requires less memory, but may lose precision (lossy compression).
    \item strategy: encode the points with a lower-dimensional representation
    \begin{itemize}
        \item $\forall x^i \in \mathbb{R}^n$, find a corresponding code vector $c^i \in \mathbb{R}^l$.
        \item find an encoding function $f(x) = c$ that produces the encoded vector, and decoding function $x \approx g(f(x))$
    \end{itemize}
    \item PCA is defined by the choice of decoding function
    \begin{itemize}
        \item let $g(c) = Dc$, where $ D \in \mathbb{R}^{n \times l}$ is the decoding transformation matrix
        \item columns of $D$ must be orthogonal to each other
        \item $D$ must have a unit norm to give us a unique solution
    \end{itemize}
    \item How do we generate the optimal $c^*$ for each input $x$?
    \begin{itemize}
        \item minimize the distance between $x$ and its reconstruction
        \item measure the "reconstruction loss" with the $L^2$ norm:
        $$ c^* = \text{argmin}_c ||x-g(c)||_2 $$
        \item we can use the squared $L^2$ norm instead:
        $$ c^* = \text{argmin}_c ||x-g(c)||_2^2 $$
        \item which simplifies to:
        \begin{align}
            (x - g(c))^T(x - g(c)) &= x^Tx - x^Tg(c) - g(c)^Tx + g(c)^Tg(c) \\
            &= x^Tx - 2x^Tg(c) + g(c)^Tg(c)
        \end{align}
        \item eliminate terms not dependent on $c$ and subsitute the definition of $g(c)$
        \begin{align}
            c^* &= \text{argmin}_c - 2x^Tg(c) + g(c)^Tg(c) \\
            &= \text{argmin}_c -2x^T Dc + c^T D^T Dc \\
            &= \text{argmin}_c -2x^T Dc + c^T I_lc \\
            &= \text{argmin}_c -2x^T Dc + c^Tc 
        \end{align}
        \item solve with vector calculus
        \begin{align}
            \nabla_c (-2x^T Dc + c^T c) &= 0 \\
            -2D^Tx + 2c &= 0 \\
            c &= D^Tx
        \end{align}
        \item So, we can encode $x$ with a matrix-vector multiply: $f(x) = D^Tx$
        \item the PCA reconstruction is then
        $$ r(x) = g(f(x)) = DD^T x$$
    \end{itemize}
    \item how do we choose the encoding matrix $D$?
    \begin{itemize}
        \item minimize the frobenius norm of the matrix of reconstruction errors:
        $$ D^* = \text{argmin}_D \sqrt{\sum_{i,j} \Big( x_j^i - r(x^i)_j \Big)^2} $$
    \end{itemize}
\end{itemize}

