\section{Linear Algebra Basics}
\subsection{Scalars, Vectors, Matrices, and Tensors}
\begin{itemize}
    \item \textbf{Scalar}: single number, specified by type $\mathbb{R}$, $\mathbb{N}$, $\mathbb{Z}$
    \item \textbf{Vector}: an array of numbers arranged in a single row or column 
    \begin{itemize}
        \item First element of $\mathbf{x}$ is $x_1$, second is $x_2$, and so on:
        $\begin{pmatrix}
            x_1 \\
            x_2 \\
            \vdots \\
            x_n \\
        \end{pmatrix}$
        \item must specify the type of numbers stored, i.e., $\mathbf{x} \in \mathbb{R}^n$, where $n$ is the number of elements/dimensionality
        \item can think of a vector as identifying a point in space; each element gives a coordinate along a different axis
        \item can index vectors with a set
        \begin{itemize}
            \item indices $1, 3, 6$ $\rightarrow$ $S = \{1, 3, 6\}$ $\rightarrow$ $x_S = \{x_1, x_3, x_6\}$
        \end{itemize}
        \item "$-$" indicates the complement of a set; $x_{-1} \rightarrow$ all elements except $-1$
    \end{itemize}
    \item \textbf{Matrix}: 2-d array of numbers
    \begin{itemize}
        \item each element is specified by two indices (row, col) instead of one
        \item $A_{m,n}$: entry at row $m$, col $n$
        \item $A_{i, :}$: all entries in the $i_{th}$ row of $A$
        \item $A_{:, j}$: all entries in the $j_{th}$ column of $A$
    \end{itemize}
    \item \textbf{Tensor}: Array with more than two axes
    \begin{itemize}
        \item $A_{i, j, k}$
    \end{itemize}
    \item \textbf{Transpose}: mirror image of a matrix across its main diagonal
    \begin{itemize}
        \item $(A^T)_{i,j} = A_{j, i}$
        \item row-column swap
    \end{itemize}
    \item \textbf{Matrix Addition}: element-wise addition of two matrices of the same size
    \item Scalar times matrix: $D = a \cdot B + c$ $\rightarrow$ $D_{i,j} = a \cdot B_{i,j} + c$ 
    \item Matrix-Vector Addition: $C = A + \mathbf{b}$ $\rightarrow$  $C_{i, j} = A_{i, j} + b_j$
    \begin{itemize}
        \item vector b is added to each row of matrix A
        \item \textbf{Broadcasting}: the copying of a vector to match the dimensions of a matrix
    \end{itemize}
\end{itemize}

\subsection{Matrix and Vector Multiplication}
\begin{itemize}
    \item \textbf{matrix product}: $C = AB$
    \begin{itemize}
        \item to be defined, $A$, must have the same number of columns as $B$ has rows.
        \item if $A$ is $m \times n$ and $B$ is $n \times p$, then $C$ is shape $m \times p$
        \item $C_{i,j} = \sum_k A_{i,k} B_{k,j}$
    \end{itemize}
    \item \textbf{Hadamard product}: element-wise multiplication of a matrix, denoted $A \circ B$
    \item \textbf{Dot Product}: $x \cdot y$ is the same dimensionality as the matrix product $x^T y$
    \begin{itemize}
        \item $C = AB$ $\rightarrow$ $C_{i,j}$ is the dot product of row $i$ of $A$ and column $j$ of $B$ 
    \end{itemize}
    \item \textbf{Matrix Operation Properties}:
    \begin{itemize}
        \item distributive: $A(B + C) = AB + AC$
        \item associative: $(AB)C = A(BC)$
        \item \textbf{not} commutative: $AB \neq BA$ in general; however vector dot product is commutative: $x^T y = y^T x$
        \item transpose of a matrix product: $(AB)^T = B^T A^T$
    \end{itemize}
    \item \textbf{System of Linear Equations}
    \begin{itemize}
        \item $Ax=b$, $A \in \mathbb{R}^{m \times n}$ is a known matrix, $b \in \mathbb{R}^m$ is a known vector, and $x \in \mathbb{R}^n$ is unkown, and we would like to solve for it.
        \item can rewrite as: $$A_{1,1}x_1 + A_{1,2}x_2 + \dots + A_{1, n}x_n = b_1$$
        $$A_{2,1}x_1 + A_{2,2}x_2 + \dots + A_{2, n}x_n = b_2$$ 
        $$\vdots$$
        $$A_{m,1}x_1 + A_{m,2}x_2 + \dots + A_{m, n}x_n = b_m$$
    \end{itemize}
\end{itemize}

\subsection{Identity and Inverse Matrices}
\begin{itemize}
    \item \textbf{Matrix Inversion}: analytic solution to the system of linear equations
    \item \textbf{Identity Matrix}: does not change a vector when multiplied by it; denoted $I_n$, formally $I_n \in \mathbb{R^{n \times n}}$ and $\forall x \in \mathbb{R}^n, I_n x = x$; takes form of 1s along the diagonal
    \item \textbf{Matrix Inverse}: $A^{-1}$ is the inverse of $A$, defined as the matrix such that $A^{-1}A = I_n$
    \item 
\end{itemize}

\subsection{Linear Dependence and Span}

\subsection{Norms}

\subsection{Special Kinds of Matrices and Vectors}

\subsection{Eigendecomposition}

\subsection{Singular Value Decomposition}

\subsection{Moore-Penrose Pseudoinverse}

\subsection{Trace Operator}

\subsection{Determinant}

\subsection{Example: PCA}

